{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# üîπ Install Dependencies\n",
        "# ==============================\n",
        "!pip -q install langchain langchain-community faiss-cpu sentence-transformers gradio pandas nltk\n",
        "\n",
        "import os, time, shutil, subprocess\n",
        "import pandas as pd\n",
        "\n",
        "# ==============================\n",
        "# üîπ Setup Ollama (Linux/Colab)\n",
        "# ==============================\n",
        "def run(cmd):\n",
        "    print(\"> \", cmd)\n",
        "    return subprocess.call(cmd, shell=True)\n",
        "\n",
        "if shutil.which(\"ollama\") is None:\n",
        "    run(\"curl -fsSL https://ollama.com/install.sh | sh\")\n",
        "\n",
        "# Start Ollama server\n",
        "_ = subprocess.Popen(\"ollama serve\", shell=True)\n",
        "time.sleep(5)\n",
        "\n",
        "# Pull required models\n",
        "run(\"ollama pull llama3.1:8b\")\n",
        "run(\"ollama pull nomic-embed-text\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "print(\"‚úÖ Setup complete\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# üîπ Config\n",
        "# ==============================\n",
        "COLLEGE_CSV = \"hyderabad_engineering_colleges.csv\"\n",
        "QA_CSV = \"hyd_college_faq_extended.csv\"\n",
        "EMBED_MODEL = \"nomic-embed-text\"\n",
        "GEN_MODEL = \"llama3.1:8b\"\n",
        "TOP_K = 10\n",
        "TEMPERATURE = 0.3\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# üîπ Load CSVs\n",
        "# ==============================\n",
        "assert os.path.exists(COLLEGE_CSV), \"College CSV not found!\"\n",
        "assert os.path.exists(QA_CSV), \"FAQ CSV not found!\"\n",
        "\n",
        "df = pd.read_csv(COLLEGE_CSV).fillna(\"\")\n",
        "faq_df = pd.read_csv(QA_CSV).fillna(\"\")\n",
        "print(\"Colleges:\", len(df), \"| FAQs:\", len(faq_df))\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# üîπ Column Normalization\n",
        "# ==============================\n",
        "CAND_COLS = {\n",
        "    \"name\": [\"college_name\",\"College_Name\",\"Name\",\"College\",\"Institute\",\"College Name\"],\n",
        "    \"area\": [\"area\",\"Area\",\"Location\",\"location\",\"Address\",\"Locality\"],\n",
        "    \"affiliation\": [\"affiliation\",\"Affiliation\",\"University\"],\n",
        "    \"accr\": [\"approved_accreditations\",\"Accreditations\",\"accreditations\",\"NAAC/NBA\",\"NAAC\",\"NBA\"],\n",
        "    \"branches\": [\"branches\",\"Branches\",\"Courses\",\"Specializations\",\"Programs\"],\n",
        "    \"fees\": [\"btech_fees_inr\",\"Fees\",\"Tuition\",\"Annual Fee\",\"Fee\",\"Fee(‚Çπ)\",\"B.Tech Fee\",\"Annual Fees (INR)\"],\n",
        "    \"website\": [\"website\",\"Website\",\"URL\"],\n",
        "    \"placements\": [\"Placements\",\"placements\",\"Highest Package\",\"Average Package\",\"Placement Stats\"],\n",
        "    \"cutoff\": [\"Cutoff\",\"cutoff\",\"TS EAMCET Cutoff\",\"JEE Cutoff\"],\n",
        "    \"rank\": [\"Rank\",\"NIRF Rank\",\"nirf_rank\",\"College Rank\"],\n",
        "    \"desc\": [\"Description\",\"Summary\",\"Overview\"],\n",
        "}\n",
        "\n",
        "def pick(r, keys):\n",
        "    for k in keys:\n",
        "        if k in r and str(r[k]).strip():\n",
        "            return str(r[k])\n",
        "    return \"\"\n",
        "\n",
        "def row_to_doc(row):\n",
        "    r = row.to_dict()\n",
        "    parts = []\n",
        "    parts.append(\"College: \" + pick(r, CAND_COLS[\"name\"]))\n",
        "    if (ar := pick(r, CAND_COLS[\"area\"])): parts.append(\"Area: \" + ar)\n",
        "    if (af := pick(r, CAND_COLS[\"affiliation\"])): parts.append(\"Affiliation: \" + af)\n",
        "    if (ac := pick(r, CAND_COLS[\"accr\"])): parts.append(\"Accreditations: \" + ac)\n",
        "    if (br := pick(r, CAND_COLS[\"branches\"])): parts.append(\"Branches: \" + br)\n",
        "    if (fe := pick(r, CAND_COLS[\"fees\"])): parts.append(\"Fees(‚Çπ): \" + fe)\n",
        "    if (cu := pick(r, CAND_COLS[\"cutoff\"])): parts.append(\"Cutoff: \" + cu)\n",
        "    if (pl := pick(r, CAND_COLS[\"placements\"])): parts.append(\"Placements: \" + pl)\n",
        "    if (rk := pick(r, CAND_COLS[\"rank\"])): parts.append(\"Rank: \" + rk)\n",
        "    if (ds := pick(r, CAND_COLS[\"desc\"])): parts.append(\"About: \" + ds)\n",
        "    if (we := pick(r, CAND_COLS[\"website\"])): parts.append(\"Website: \" + we)\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "docs_text = [row_to_doc(row) for _, row in df.iterrows()]\n",
        "faq_texts = [f\"Q: {row['prompt']} | A: {row['answer']}\" for _, row in faq_df.iterrows()]\n",
        "all_docs = docs_text + faq_texts\n",
        "print(\"üìÑ Documents prepared:\", len(all_docs))\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# üîπ Embeddings & FAISS\n",
        "# ==============================\n",
        "from langchain_community.embeddings import OllamaEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "try:\n",
        "    emb = OllamaEmbeddings(model=EMBED_MODEL)\n",
        "    _ = emb.embed_query(\"hi\")\n",
        "    print(\"‚úÖ Using OllamaEmbeddings:\", EMBED_MODEL)\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Ollama failed, using HuggingFace:\", e)\n",
        "    emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vs = FAISS.from_texts(all_docs, emb)\n",
        "vs.save_local(\"faiss_hyd_colleges\")\n",
        "print(\"‚úÖ Vector index built & saved\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# üîπ RAG Chain\n",
        "# ==============================\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "vs = FAISS.load_local(\"faiss_hyd_colleges\", emb, allow_dangerous_deserialization=True)\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": TOP_K})\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are a friendly Hyderabad B.Tech admissions advisor.\\n\"\n",
        "    \"- If an answer is not found in the knowledge base, say 'Not available in database. Please check official college websites.'\\n\"\n",
        "    \"- Never invent colleges or info outside the retrieved data.\\n\"\n",
        "    \"- Answer naturally & conversationally.\\n\"\n",
        "    \"- Use bullet points for multi-item answers.\\n\"\n",
        "    \"- Remind that fees, cutoffs, and placements vary yearly.\\n\"\n",
        ")\n",
        "\n",
        "qa_prompt = PromptTemplate.from_template(\n",
        "    \"{system_prompt}\\nChat history:\\n{chat_history}\\n\\nContext:\\n{context}\\n\\nUser: {question}\\nAssistant:\"\n",
        ")\n",
        "\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "\n",
        "# Streaming handler (optional: shows tokens in console)\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self):\n",
        "        self.buffer = \"\"\n",
        "    def on_llm_new_token(self, token: str, **kwargs):\n",
        "        print(token, end=\"\", flush=True)  # prints tokens live\n",
        "        self.buffer += token\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=GEN_MODEL,\n",
        "    temperature=TEMPERATURE,\n",
        "    streaming=True,\n",
        "    callbacks=[StreamHandler()]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": qa_prompt.partial(system_prompt=system_prompt)},\n",
        ")\n",
        "\n",
        "print(\"‚úÖ RAG chain ready\")\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# üîπ Gradio Chat UI\n",
        "# ==============================\n",
        "import gradio as gr\n",
        "\n",
        "# üîπ Streaming chat function\n",
        "def chat_fn(message, history):\n",
        "    response = \"\"\n",
        "    try:\n",
        "        # Build context manually (instead of qa chain)\n",
        "        chat_history = \"\\n\".join([f\"User: {u}\\nAssistant: {a}\" for u, a in history])\n",
        "        docs = retriever.get_relevant_documents(message)\n",
        "        context = \"\\n\".join([d.page_content for d in docs])\n",
        "        full_prompt = qa_prompt.format(\n",
        "            system_prompt=system_prompt,\n",
        "            chat_history=chat_history,\n",
        "            context=context,\n",
        "            question=message,\n",
        "        )\n",
        "\n",
        "        # üîπ Stream raw LLM tokens\n",
        "        for chunk in llm.stream(full_prompt):\n",
        "            token = chunk.content\n",
        "            response += token\n",
        "            yield response\n",
        "\n",
        "        memory.chat_memory.add_user_message(message)\n",
        "        memory.chat_memory.add_ai_message(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        yield f\"‚ö†Ô∏è Error: {str(e)}\"\n",
        "\n",
        "\n",
        "# üîπ Gradio UI\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"<h2 style='text-align:center;'>üéì Engg Assist - Hyderabad B.Tech Chatbot ü§ñ</h2>\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(placeholder=\"Ask about colleges, fees, cutoffs, placements...\", autofocus=True)\n",
        "    clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    # ‚úÖ Respond function with generator\n",
        "    def respond(user_msg, chat_history):\n",
        "        chat_history.append((user_msg, \"\"))   # reserve bot reply slot\n",
        "        for partial in chat_fn(user_msg, chat_history):\n",
        "            chat_history[-1] = (user_msg, partial)\n",
        "            yield chat_history, \"\"   # stream updates to UI\n",
        "\n",
        "    # Bind events\n",
        "    msg.submit(respond, [msg, chatbot], [chatbot, msg])\n",
        "    clear.click(lambda: [], None, chatbot)\n",
        "\n",
        "# üîπ Launch\n",
        "demo.launch(share=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "XrSG0829syu1",
        "outputId": "d6eaf6a0-172c-4f1b-db25-88614bd8cf9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">  ollama pull llama3.1:8b\n",
            ">  ollama pull nomic-embed-text\n",
            "‚úÖ Setup complete\n",
            "Colleges: 42 | FAQs: 236\n",
            "üìÑ Documents prepared: 278\n",
            "‚úÖ Using OllamaEmbeddings: nomic-embed-text\n",
            "‚úÖ Vector index built & saved\n",
            "‚úÖ RAG chain ready\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-942073553.py:207: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f0b07a659c1bcba684.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f0b07a659c1bcba684.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vfwE0TjwxUw_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}